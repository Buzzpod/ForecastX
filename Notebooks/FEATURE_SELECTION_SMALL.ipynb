{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform various forms of analysis to determine which features that are available to us may be most useful when training multivariate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from Functions import tsPlot\n",
    "from copy import deepcopy as dc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from skopt import gp_minimize\n",
    "from tqdm import tqdm  # for progress bar\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a pandas DataFrame\n",
    "df_ret = pd.read_csv('../DataManagement/daily_data.csv', parse_dates=['DATE'], index_col='DATE')\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2018-06-30'\n",
    "end_date = '2023-06-30'\n",
    "\n",
    "# Slice the DataFrame for the desired date range\n",
    "df_ret = df_ret.loc[start_date:end_date].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data, we follow the exact same steps as we did when building our multivariate LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function that transforms the data into the format requred by the LSTM with parameters for lookback period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_lstm(data, n_steps, column):\n",
    "    column_names = [column]\n",
    "    data = dc(data)  # make deep copy of the input data\n",
    "\n",
    "    for i in range(1, n_steps+1):\n",
    "        column_name = f'{column}(t-{i})'\n",
    "        column_names.append(column_name)\n",
    "        data[column_name] = data[column].shift(i)\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.loc[:, data.columns.intersection(column_names)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we are working with multivariate data, we need to be careful to make sure that our X matrix follows the structure laid out in the diagram found in the README file. We will effectively call the prepare_data_lstm function on each feature seperaterly to generate the sequences and then perform some transformations afterwards to combine the data into a single X matrix as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 7\n",
    "\n",
    "# Same as the univariate case\n",
    "shifted_close = prepare_data_lstm(df_ret, lookback, 'NVDA_CLOSE')    \n",
    "\n",
    "# New: now we also perform the same procedure on each of the additional features that we wish to include in our X matrix\n",
    "shifted_open = prepare_data_lstm(df_ret, lookback, 'NVDA_OPEN')\n",
    "shifted_high = prepare_data_lstm(df_ret, lookback, 'NVDA_HIGH')\n",
    "shifted_low = prepare_data_lstm(df_ret, lookback, 'NVDA_LOW')\n",
    "shifted_volume = prepare_data_lstm(df_ret, lookback, 'NVDA_VOLUME')\n",
    "\n",
    "# Now we convert the dataframes into numpy matrices\n",
    "shifted_close_np = shifted_close.to_numpy()\n",
    "shifted_open_np = shifted_open.to_numpy()\n",
    "shifted_high_np = shifted_high.to_numpy()\n",
    "shifted_low_np = shifted_low.to_numpy()\n",
    "shifted_volume_np = shifted_volume.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform standardisation on the dataset. We have to be careful here because some variables are recorded on very different scales, for example, volume figures are magnitudes larger than open, high, low and close figures. As a result, we will scale volume seperately to the other variables so that we do not introduce bias into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler for price data\n",
    "price_scaler = MinMaxScaler(feature_range=(-1,1))    # scale to range -1 to 1\n",
    "\n",
    "# Scaler for volume data\n",
    "volume_scaler = MinMaxScaler(feature_range=(-1,1))    # scale to range -1 to 1\n",
    "\n",
    "# Scale the price data\n",
    "shifted_open_np_scaled = price_scaler.fit_transform(shifted_open_np)\n",
    "shifted_high_np_scaled = price_scaler.fit_transform(shifted_high_np)\n",
    "shifted_low_np_scaled = price_scaler.fit_transform(shifted_low_np)\n",
    "shifted_close_np_scaled = price_scaler.fit_transform(shifted_close_np)\n",
    "\n",
    "# Scale the volume data\n",
    "shifted_volume_np_scaled = volume_scaler.fit_transform(shifted_volume_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a quick check to make sure the shapes of out numpy matrices match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1251, 8) (1251, 8)\n"
     ]
    }
   ],
   "source": [
    "print(shifted_close_np_scaled.shape, shifted_volume_np_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the standardised returns into our X and y vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our y vector does not change in the multivariate case since we are still predicting the close prices\n",
    "y = shifted_close_np_scaled[:, 0]\n",
    "\n",
    "# Our X matrix does change though as we need to add additional dimensions to store the extra variables\n",
    "# We start by slicing out the time t column from each of the X components\n",
    "X_close = shifted_close_np_scaled[:, 1:]\n",
    "X_open = shifted_open_np_scaled[:, 1:]\n",
    "X_high = shifted_high_np_scaled[:, 1:]\n",
    "X_low = shifted_low_np_scaled[:, 1:]\n",
    "X_volume = shifted_volume_np_scaled[:, 1:]\n",
    "\n",
    "# Then we individually \"flip\" each X component so that it goes, for example, t-7, t-6, t-5....\n",
    "X_close = dc(np.flip(X_close, axis=1))\n",
    "X_open = dc(np.flip(X_open, axis=1))\n",
    "X_high = dc(np.flip(X_high, axis=1))\n",
    "X_low = dc(np.flip(X_low, axis=1))\n",
    "X_volume = dc(np.flip(X_volume, axis=1))\n",
    "\n",
    "# Finally, we combine each X component into a single X matrix with the shape (samples, time steps, features) i.e. (1251, 7, 4) in this case\n",
    "X = np.stack((X_close, X_open, X_high, X_low, X_volume), axis=-1)\n",
    "\n",
    "# Also, y currently has shape (1251) but it needs to have shape (1251, 1) in this framework\n",
    "y = y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that the shape of y and X are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1251, 1), (1251, 7, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform a train/test split where the first 95% of the data is used for training/validation and the last 5% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7, 5) (188, 7, 5) (63, 7, 5) (1000, 1) (188, 1) (63, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the split proportions first\n",
    "train_ratio = 0.80  # 80% of data for training\n",
    "valid_ratio = 0.15  # 15% of data for validation\n",
    "test_ratio = 0.05   # 5% of data for testing\n",
    "\n",
    "# First split: separate out the test set\n",
    "train_valid_index = int(len(X) * (train_ratio + valid_ratio))\n",
    "X_train_valid, X_test = X[:train_valid_index], X[train_valid_index:]\n",
    "y_train_valid, y_test = y[:train_valid_index], y[train_valid_index:]\n",
    "\n",
    "# Second split: separate out the validation set from the remaining data\n",
    "train_index = int(len(X_train_valid) * train_ratio / (train_ratio + valid_ratio))\n",
    "X_train, X_valid = X_train_valid[:train_index], X_train_valid[train_index:]\n",
    "y_train, y_valid = y_train_valid[:train_index], y_train_valid[train_index:]\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the matrices into tensor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).float()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).float()\n",
    "X_valid, y_valid = torch.tensor(X_valid).float(), torch.tensor(y_valid).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a class that we will use to turn these individual matrices into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "    \n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "valid_dataset = TimeSeriesDataset(X_valid, y_valid)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap our datasets in data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the batches to our compute device (GPU:0 in this case as we defined in the 'setup' section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7, 5]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple yet effective method to determine if a feature should be included in the model. Correlation measures the degree of relationship between two variables. If a feature is highly correlated with the target variable, it's often beneficial to include it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Close      Open      High       Low    Volume\n",
      "Close   1.000000  0.998624  0.999374  0.999384 -0.030139\n",
      "Open    0.998624  1.000000  0.999446  0.999430 -0.032336\n",
      "High    0.999374  0.999446  1.000000  0.999323 -0.021220\n",
      "Low     0.999384  0.999430  0.999323  1.000000 -0.040430\n",
      "Volume -0.030139 -0.032336 -0.021220 -0.040430  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Select t-1 timestep for each feature\n",
    "t_1_close = shifted_close_np_scaled[:, -1]\n",
    "t_1_open = shifted_open_np_scaled[:, -1]\n",
    "t_1_high = shifted_high_np_scaled[:, -1]\n",
    "t_1_low = shifted_low_np_scaled[:, -1]\n",
    "t_1_volume = shifted_volume_np_scaled[:, -1]\n",
    "\n",
    "# Combine into a 2D array\n",
    "data_t_1 = np.stack((t_1_close, t_1_open, t_1_high, t_1_low, t_1_volume), axis=-1)\n",
    "\n",
    "# Create a dataframe\n",
    "df_t_1 = pd.DataFrame(data_t_1, columns=['Close', 'Open', 'High', 'Low', 'Volume'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix_t_1 = df_t_1.corr()\n",
    "print(correlation_matrix_t_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method involves training the model with all the features first, then for each feature, the values are permuted in the validation data and the decrease in the model score is computed. The features which cause the largest decrease in score are considered the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to understand the importance of multiple forms of time series data, i.e. open, close, high, low and volume on the performance of an LSTM model that we have already trained.\n",
    "\n",
    "As a result, the first step is to load in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(5, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimal Parameters\n",
    "hidden_size_optimal = 128\n",
    "num_stacked_layers_optimal = 2\n",
    "dropout_optimal = 0.0\n",
    "\n",
    "# Fixed parameters\n",
    "input_size = 5\n",
    "num_epochs = 20\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define custom LSTM class (same as before)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model_optimal = LSTM(input_size, hidden_size_optimal, num_stacked_layers_optimal, dropout_optimal)\n",
    "model_optimal.to(device)\n",
    "\n",
    "# Load the saved model\n",
    "model_optimal.load_state_dict(torch.load(\"../Models/MvLSTM/24-07-2023_14-42-13/MvLSTM.pth\"))    # choose which saved model to load\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model_optimal.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New we have loaded the LSTM model, we can implement our permutation importance procedure using the eli5 library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Our features are 'Close', 'Open', 'High', 'Low', 'Volume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance: [-0.06099544, -0.02909847, -0.042314332, -0.0385322, 3.360957e-05]\n"
     ]
    }
   ],
   "source": [
    "def score(X, y):\n",
    "    model_optimal.eval()\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model_optimal(X).detach().cpu().numpy()\n",
    "    return np.sqrt(mean_squared_error(y.cpu().numpy(), y_pred))\n",
    "\n",
    "baseline = score(X_valid, y_valid)\n",
    "imp = []\n",
    "for feature in range(X_valid.shape[2]):  # Iterate over features, not sequence steps\n",
    "    save = X_valid[:, :, feature].clone()  # Use clone for PyTorch tensor\n",
    "    X_valid[:, :, feature] = torch.tensor(np.random.permutation(X_valid[:, :, feature].cpu().numpy()), device=device)  # Convert back to tensor after permutation\n",
    "    m = score(X_valid, y_valid)\n",
    "    X_valid[:, :, feature] = save  # Restore original values\n",
    "    imp.append(baseline - m)\n",
    "\n",
    "# Print the feature importance\n",
    "print('Feature importance:', imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation feature importance is a technique used to determine the most important features in our dataset. It works by shuffling the values of each feature and measuring how much the performance of the model decreases. The more the performance decreases, the more important the feature is.\n",
    "\n",
    "Now to interpret these specific results:\n",
    "\n",
    "* **Close:** \n",
    "* **Open:**\n",
    "* **High:**\n",
    "* **Low:**\n",
    "* **Volume:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
